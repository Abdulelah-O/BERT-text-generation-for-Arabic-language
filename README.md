# ğŸ“ BERT-Arabic_Text_Generation: Fine-tuning AraBERT for Arabic Text Completion

This project fine-tunes a pre-trained Arabic BERT model, AraBERT, for the task of Masked Language Modeling (MLM). The model is trained on a custom Arabic text dataset to learn contextual representations specific to the provided data. The fine-tuned model is then used to predict and complete masked tokens in Arabic sentences, effectively performing a form of text generation.

---

## ğŸ“‚ Project Structure

.
â”œâ”€â”€ nlp_dataset/ # Dataset (not uploaded â€“ see instructions below)
â”œâ”€â”€ results/ # Training results & logs (not uploaded)
â”œâ”€â”€ trained_model/ # Trained model weights (not uploaded)
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ data_preprocessing.py # Arabic text preprocessing functions
â”‚ â”œâ”€â”€ model_training.py # Model training script
â”‚ â”œâ”€â”€ text_generation.py # Text generation script
â”‚ â””â”€â”€ utils.py # Helper utilities
â”œâ”€â”€ BERT-Arabic_Text_Genration.ipynb # Main notebook (step-by-step demo)
â”œâ”€â”€ requirements.txt # Dependencies
â”œâ”€â”€ .gitignore # Exclude large files/folders
â””â”€â”€ README.md # Project documentation


- nlp_dataset/: Contains the raw Arabic text data, organized into different categories. This data is used for training the model.

- trained_model/: This directory stores the fine-tuned AraBERT model and its tokenizer after the training process is complete.

- results/: This folder holds the training outputs, including model checkpoints and training logs.

- BERT-Arabic_Text_Generation.ipynb: The main Jupyter Notebook containing the code for data loading, model training, and text completion.
