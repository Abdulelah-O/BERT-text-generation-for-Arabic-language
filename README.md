# ğŸ“ BERT-Arabic_Text_Generation: Fine-tuning AraBERT for Arabic Text Completion

This project fine-tunes a pre-trained Arabic BERT model, AraBERT, for the task of Masked Language Modeling (MLM). The model is trained on a custom Arabic text dataset to learn contextual representations specific to the provided data. The fine-tuned model is then used to predict and complete masked tokens in Arabic sentences, effectively performing a form of text generation.

---

## ğŸ“‚ Project Structure

.
â”œâ”€â”€ nlp_dataset/         
â”‚     â”œâ”€â”€  articles-culture/    
â”‚     â”œâ”€â”€  articles-economy/  
â”‚     â”œâ”€â”€  ...    
â”œâ”€â”€ trained_model/ 
â”‚     â”œâ”€â”€ config.json
â”‚     â”œâ”€â”€ pytorch_model.bin  
â”‚     â”œâ”€â”€ tokenizer.json
â”‚     â”œâ”€â”€ special_token_map.json
â”‚     â”œâ”€â”€ ...
â”œâ”€â”€ resutls/
â”‚     â”œâ”€â”€ checkpoint-1000/
â”‚     â”œâ”€â”€ chechpoint-2000/
â”‚     â”œâ”€â”€ ...
â”‚
â””â”€â”€ BERT-Arabic_Text_Generation.ipynb

- nlp_dataset/: Contains the raw Arabic text data, organized into different categories. This data is used for training the model.

- trained_model/: This directory stores the fine-tuned AraBERT model and its tokenizer after the training process is complete.

- results/: This folder holds the training outputs, including model checkpoints and training logs.

- BERT-Arabic_Text_Generation.ipynb: The main Jupyter Notebook containing the code for data loading, model training, and text completion.
